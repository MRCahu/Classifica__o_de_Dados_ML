{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Completo de Machine Learning: Classifica√ß√£o de Reclama√ß√µes de Consumidores\n",
    "\n",
    "## üìã Objetivo do Projeto\n",
    "\n",
    "Este projeto desenvolve um sistema de **classifica√ß√£o multiclasse** para categorizar automaticamente reclama√ß√µes de consumidores em diferentes tipos de problemas. O contexto √© de **servi√ßo p√∫blico/administrativo**, simulando um sistema que poderia ser usado por √≥rg√£os de defesa do consumidor.\n",
    "\n",
    "### üéØ Objetivos Espec√≠ficos:\n",
    "- Classificar reclama√ß√µes em 4 categorias principais\n",
    "- Aplicar boas pr√°ticas de Machine Learning\n",
    "- Demonstrar o pipeline completo de um projeto de ML\n",
    "- Criar modelos reutiliz√°veis para produ√ß√£o\n",
    "\n",
    "### üìä Metodologia:\n",
    "1. **An√°lise Explorat√≥ria de Dados (EDA)**\n",
    "2. **Pr√©-processamento e Feature Engineering**\n",
    "3. **Treinamento de M√∫ltiplos Modelos**\n",
    "4. **Avalia√ß√£o e Compara√ß√£o**\n",
    "5. **Valida√ß√£o e Ajuste de Hiperpar√¢metros**\n",
    "6. **Predi√ß√£o em Novos Dados**\n",
    "7. **Salvamento de Modelos**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üìö Importa√ß√£o de Bibliotecas e Configura√ß√£o\n",
    "\n",
    "Primeiro, vamos importar todas as bibliotecas necess√°rias para nosso projeto. Cada biblioteca tem um prop√≥sito espec√≠fico no pipeline de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipula√ß√£o de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning - Pr√©-processamento\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Machine Learning - Modelos\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Machine Learning - Avalia√ß√£o\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "\n",
    "# Persist√™ncia de modelos\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
    "print(f\"üìÖ Data de execu√ß√£o: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üóÇÔ∏è Cria√ß√£o e Carregamento do Dataset\n",
    "\n",
    "Como estamos trabalhando com um contexto educativo, vamos criar um dataset sint√©tico realista de reclama√ß√µes de consumidores. Este dataset simula dados que poderiam ser encontrados em √≥rg√£os como PROCON ou plataformas de atendimento ao consumidor.\n",
    "\n",
    "### üìù Estrutura do Dataset:\n",
    "- **Texto da reclama√ß√£o**: Descri√ß√£o do problema\n",
    "- **Categoria**: Tipo de problema (4 classes)\n",
    "- **Empresa**: Nome da empresa reclamada\n",
    "- **Estado**: Estado onde ocorreu o problema\n",
    "- **Valor**: Valor envolvido na reclama√ß√£o\n",
    "- **Tempo_resposta**: Tempo para resposta da empresa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria√ß√£o de um dataset sint√©tico realista\n",
    "np.random.seed(42)\n",
    "\n",
    "# Definindo as categorias de reclama√ß√µes (4 classes)\n",
    "categorias = {\n",
    "    'Produto Defeituoso': [\n",
    "        'Produto chegou com defeito de f√°brica',\n",
    "        'Aparelho parou de funcionar ap√≥s poucos dias',\n",
    "        'Produto n√£o corresponde √† descri√ß√£o',\n",
    "        'Defeito na tela do celular',\n",
    "        'Produto veio danificado na entrega',\n",
    "        'Qualidade inferior ao esperado',\n",
    "        'Produto apresentou problemas logo ap√≥s a compra'\n",
    "    ],\n",
    "    'Atendimento': [\n",
    "        'Atendimento muito demorado no telefone',\n",
    "        'Funcion√°rio foi grosseiro e mal educado',\n",
    "        'N√£o consegui resolver meu problema',\n",
    "        'Atendente n√£o soube me informar',\n",
    "        'Fui mal atendido na loja f√≠sica',\n",
    "        'Demora excessiva para resposta',\n",
    "        'Atendimento de baixa qualidade'\n",
    "    ],\n",
    "    'Cobran√ßa Indevida': [\n",
    "        'Cobraram valor a mais na fatura',\n",
    "        'Taxa n√£o informada na contrata√ß√£o',\n",
    "        'Cobran√ßa duplicada no cart√£o',\n",
    "        'Valor diferente do acordado',\n",
    "        'Cobran√ßa ap√≥s cancelamento do servi√ßo',\n",
    "        'Juros abusivos aplicados',\n",
    "        'Cobran√ßa de servi√ßo n√£o solicitado'\n",
    "    ],\n",
    "    'Entrega': [\n",
    "        'Produto n√£o foi entregue no prazo',\n",
    "        'Entrega foi feita no endere√ßo errado',\n",
    "        'Produto chegou muito atrasado',\n",
    "        'N√£o recebi o produto comprado',\n",
    "        'Entregador foi mal educado',\n",
    "        'Produto foi entregue danificado',\n",
    "        'Prazo de entrega n√£o foi cumprido'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Empresas fict√≠cias\n",
    "empresas = [\n",
    "    'TechMart', 'SuperCompras', 'MegaStore', 'FastDelivery', \n",
    "    'EletroMax', 'ShopOnline', 'QuickBuy', 'BestPrice',\n",
    "    'TopQuality', 'ExpressShop'\n",
    "]\n",
    "\n",
    "# Estados brasileiros\n",
    "estados = [\n",
    "    'SP', 'RJ', 'MG', 'RS', 'PR', 'SC', 'BA', 'GO', \n",
    "    'PE', 'CE', 'DF', 'ES', 'PB', 'RN', 'MT'\n",
    "]\n",
    "\n",
    "# Gerando dados sint√©ticos\n",
    "dados = []\n",
    "n_samples = 2000\n",
    "\n",
    "for i in range(n_samples):\n",
    "    categoria = np.random.choice(list(categorias.keys()))\n",
    "    texto = np.random.choice(categorias[categoria])\n",
    "    empresa = np.random.choice(empresas)\n",
    "    estado = np.random.choice(estados)\n",
    "    \n",
    "    # Valores baseados na categoria\n",
    "    if categoria == 'Produto Defeituoso':\n",
    "        valor = np.random.uniform(50, 2000)\n",
    "        tempo_resposta = np.random.uniform(1, 15)\n",
    "    elif categoria == 'Cobran√ßa Indevida':\n",
    "        valor = np.random.uniform(20, 500)\n",
    "        tempo_resposta = np.random.uniform(1, 10)\n",
    "    elif categoria == 'Entrega':\n",
    "        valor = np.random.uniform(30, 800)\n",
    "        tempo_resposta = np.random.uniform(1, 7)\n",
    "    else:  # Atendimento\n",
    "        valor = np.random.uniform(0, 100)\n",
    "        tempo_resposta = np.random.uniform(1, 20)\n",
    "    \n",
    "    dados.append({\n",
    "        'texto_reclamacao': texto,\n",
    "        'categoria': categoria,\n",
    "        'empresa': empresa,\n",
    "        'estado': estado,\n",
    "        'valor_envolvido': round(valor, 2),\n",
    "        'tempo_resposta_dias': round(tempo_resposta, 1)\n",
    "    })\n",
    "\n",
    "# Criando DataFrame\n",
    "df = pd.DataFrame(dados)\n",
    "\n",
    "print(f\"‚úÖ Dataset criado com sucesso!\")\n",
    "print(f\"üìä Dimens√µes: {df.shape[0]} linhas e {df.shape[1]} colunas\")\n",
    "print(f\"üéØ Classes: {df['categoria'].nunique()} categorias\")\n",
    "\n",
    "# Salvando o dataset\n",
    "df.to_csv('/home/ubuntu/reclamacoes_dataset.csv', index=False)\n",
    "print(\"üíæ Dataset salvo como 'reclamacoes_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üîç An√°lise Explorat√≥ria de Dados (EDA)\n",
    "\n",
    "A **An√°lise Explorat√≥ria de Dados** √© uma etapa fundamental em qualquer projeto de Machine Learning. Ela nos permite:\n",
    "\n",
    "- üéØ **Entender a distribui√ß√£o dos dados**\n",
    "- üîç **Identificar padr√µes e anomalias**\n",
    "- üìä **Verificar balanceamento das classes**\n",
    "- üßπ **Detectar dados faltantes ou inconsistentes**\n",
    "- üí° **Gerar insights para feature engineering**\n",
    "\n",
    "### 3.1 Vis√£o Geral dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informa√ß√µes b√°sicas do dataset\n",
    "print(\"üìã INFORMA√á√ïES GERAIS DO DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dimens√µes: {df.shape}\")\n",
    "print(f\"Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "print(\"\\nüìä TIPOS DE DADOS:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nüîç PRIMEIRAS 5 LINHAS:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nüìà ESTAT√çSTICAS DESCRITIVAS:\")\n",
    "display(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando dados faltantes\n",
    "print(\"üîç AN√ÅLISE DE DADOS FALTANTES\")\n",
    "print(\"=\" * 40)\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valores Faltantes': missing_data,\n",
    "    'Percentual (%)': missing_percent\n",
    "})\n",
    "\n",
    "print(missing_df)\n",
    "\n",
    "if missing_data.sum() == 0:\n",
    "    print(\"\\n‚úÖ Excelente! N√£o h√° dados faltantes no dataset.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Aten√ß√£o: Existem dados faltantes que precisam ser tratados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 An√°lise da Vari√°vel Target (Categoria)\n",
    "\n",
    "A an√°lise da vari√°vel target √© crucial para entender:\n",
    "- **Balanceamento das classes**\n",
    "- **Distribui√ß√£o dos dados**\n",
    "- **Poss√≠veis desafios de classifica√ß√£o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise da distribui√ß√£o das categorias\n",
    "print(\"üéØ AN√ÅLISE DA VARI√ÅVEL TARGET (CATEGORIA)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "categoria_counts = df['categoria'].value_counts()\n",
    "categoria_percent = df['categoria'].value_counts(normalize=True) * 100\n",
    "\n",
    "categoria_analysis = pd.DataFrame({\n",
    "    'Quantidade': categoria_counts,\n",
    "    'Percentual (%)': categoria_percent.round(2)\n",
    "})\n",
    "\n",
    "print(categoria_analysis)\n",
    "\n",
    "# Verificando balanceamento\n",
    "max_class = categoria_percent.max()\n",
    "min_class = categoria_percent.min()\n",
    "balance_ratio = max_class / min_class\n",
    "\n",
    "print(f\"\\nüìä AN√ÅLISE DE BALANCEAMENTO:\")\n",
    "print(f\"Classe mais frequente: {max_class:.1f}%\")\n",
    "print(f\"Classe menos frequente: {min_class:.1f}%\")\n",
    "print(f\"Raz√£o de desbalanceamento: {balance_ratio:.2f}\")\n",
    "\n",
    "if balance_ratio <= 2:\n",
    "    print(\"‚úÖ Dataset bem balanceado!\")\n",
    "elif balance_ratio <= 4:\n",
    "    print(\"‚ö†Ô∏è Leve desbalanceamento - monitorar performance\")\n",
    "else:\n",
    "    print(\"üö® Dataset desbalanceado - considerar t√©cnicas de balanceamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o da distribui√ß√£o das categorias\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['Distribui√ß√£o das Categorias', 'Distribui√ß√£o Percentual'],\n",
    "    specs=[[{'type': 'bar'}, {'type': 'pie'}]]\n",
    ")\n",
    "\n",
    "# Gr√°fico de barras\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=categoria_counts.index,\n",
    "        y=categoria_counts.values,\n",
    "        text=categoria_counts.values,\n",
    "        textposition='auto',\n",
    "        name='Quantidade',\n",
    "        marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Gr√°fico de pizza\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=categoria_counts.index,\n",
    "        values=categoria_counts.values,\n",
    "        textinfo='label+percent',\n",
    "        marker_colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"üìä An√°lise da Distribui√ß√£o das Categorias de Reclama√ß√µes\",\n",
    "    title_x=0.5,\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Categoria\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Quantidade\", row=1, col=1)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Salvando o gr√°fico\n",
    "fig.write_html(\"/home/ubuntu/distribuicao_categorias.html\")\n",
    "print(\"üíæ Gr√°fico salvo como 'distribuicao_categorias.html'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 An√°lise das Vari√°veis Num√©ricas\n",
    "\n",
    "Vamos analisar as vari√°veis num√©ricas para entender:\n",
    "- **Distribui√ß√µes e outliers**\n",
    "- **Correla√ß√µes entre vari√°veis**\n",
    "- **Padr√µes por categoria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise das vari√°veis num√©ricas\n",
    "numeric_cols = ['valor_envolvido', 'tempo_resposta_dias']\n",
    "\n",
    "print(\"üìä AN√ÅLISE DAS VARI√ÅVEIS NUM√âRICAS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for col in numeric_cols:\n",
    "    print(f\"\\nüîç {col.upper()}:\")\n",
    "    print(f\"M√©dia: {df[col].mean():.2f}\")\n",
    "    print(f\"Mediana: {df[col].median():.2f}\")\n",
    "    print(f\"Desvio Padr√£o: {df[col].std():.2f}\")\n",
    "    print(f\"M√≠nimo: {df[col].min():.2f}\")\n",
    "    print(f\"M√°ximo: {df[col].max():.2f}\")\n",
    "    \n",
    "    # Detectando outliers usando IQR\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    print(f\"Outliers detectados: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o das vari√°veis num√©ricas por categoria\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        'Distribui√ß√£o do Valor Envolvido por Categoria',\n",
    "        'Distribui√ß√£o do Tempo de Resposta por Categoria',\n",
    "        'Boxplot - Valor Envolvido',\n",
    "        'Boxplot - Tempo de Resposta'\n",
    "    ],\n",
    "    specs=[[{'type': 'box'}, {'type': 'box'}],\n",
    "           [{'type': 'violin'}, {'type': 'violin'}]]\n",
    ")\n",
    "\n",
    "# Cores para cada categoria\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "categories = df['categoria'].unique()\n",
    "\n",
    "# Boxplots\n",
    "for i, cat in enumerate(categories):\n",
    "    data_cat = df[df['categoria'] == cat]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=data_cat['valor_envolvido'],\n",
    "            name=cat,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=data_cat['tempo_resposta_dias'],\n",
    "            name=cat,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Violin plots\n",
    "    fig.add_trace(\n",
    "        go.Violin(\n",
    "            y=data_cat['valor_envolvido'],\n",
    "            name=cat,\n",
    "            fillcolor=colors[i],\n",
    "            line_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Violin(\n",
    "            y=data_cat['tempo_resposta_dias'],\n",
    "            name=cat,\n",
    "            fillcolor=colors[i],\n",
    "            line_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"üìä An√°lise das Vari√°veis Num√©ricas por Categoria\",\n",
    "    title_x=0.5,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Valor (R$)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Tempo (dias)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Valor (R$)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Tempo (dias)\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Salvando o gr√°fico\n",
    "fig.write_html(\"/home/ubuntu/analise_variaveis_numericas.html\")\n",
    "print(\"üíæ Gr√°fico salvo como 'analise_variaveis_numericas.html'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 An√°lise das Vari√°veis Categ√≥ricas\n",
    "\n",
    "Vamos analisar as vari√°veis categ√≥ricas (empresa e estado) para entender sua distribui√ß√£o e rela√ß√£o com a vari√°vel target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise das vari√°veis categ√≥ricas\n",
    "categorical_cols = ['empresa', 'estado']\n",
    "\n",
    "print(\"üè¢ AN√ÅLISE DAS VARI√ÅVEIS CATEG√ìRICAS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nüìä {col.upper()}:\")\n",
    "    print(f\"Valores √∫nicos: {df[col].nunique()}\")\n",
    "    print(f\"Valor mais frequente: {df[col].mode()[0]} ({df[col].value_counts().iloc[0]} ocorr√™ncias)\")\n",
    "    \n",
    "    # Top 5 valores\n",
    "    print(f\"\\nTop 5 {col}s:\")\n",
    "    top_values = df[col].value_counts().head()\n",
    "    for idx, (value, count) in enumerate(top_values.items(), 1):\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"{idx}. {value}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o da rela√ß√£o entre vari√°veis categ√≥ricas e target\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        'Distribui√ß√£o por Empresa',\n",
    "        'Distribui√ß√£o por Estado',\n",
    "        'Heatmap: Empresa vs Categoria',\n",
    "        'Heatmap: Estado vs Categoria'\n",
    "    ],\n",
    "    specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
    "           [{'type': 'heatmap'}, {'type': 'heatmap'}]]\n",
    ")\n",
    "\n",
    "# Distribui√ß√£o por empresa\n",
    "empresa_counts = df['empresa'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=empresa_counts.index,\n",
    "        y=empresa_counts.values,\n",
    "        text=empresa_counts.values,\n",
    "        textposition='auto',\n",
    "        marker_color='#FF6B6B',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Distribui√ß√£o por estado (top 10)\n",
    "estado_counts = df['estado'].value_counts().head(10)\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=estado_counts.index,\n",
    "        y=estado_counts.values,\n",
    "        text=estado_counts.values,\n",
    "        textposition='auto',\n",
    "        marker_color='#4ECDC4',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Heatmap empresa vs categoria\n",
    "empresa_categoria = pd.crosstab(df['empresa'], df['categoria'])\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=empresa_categoria.values,\n",
    "        x=empresa_categoria.columns,\n",
    "        y=empresa_categoria.index,\n",
    "        colorscale='Viridis',\n",
    "        showscale=False\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Heatmap estado vs categoria (top 10 estados)\n",
    "top_estados = df['estado'].value_counts().head(10).index\n",
    "df_top_estados = df[df['estado'].isin(top_estados)]\n",
    "estado_categoria = pd.crosstab(df_top_estados['estado'], df_top_estados['categoria'])\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=estado_categoria.values,\n",
    "        x=estado_categoria.columns,\n",
    "        y=estado_categoria.index,\n",
    "        colorscale='Plasma',\n",
    "        showscale=False\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"üè¢ An√°lise das Vari√°veis Categ√≥ricas\",\n",
    "    title_x=0.5,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Empresa\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Estado\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Quantidade\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Quantidade\", row=1, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Salvando o gr√°fico\n",
    "fig.write_html(\"/home/ubuntu/analise_variaveis_categoricas.html\")\n",
    "print(\"üíæ Gr√°fico salvo como 'analise_variaveis_categoricas.html'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Matriz de Correla√ß√£o\n",
    "\n",
    "Vamos analisar as correla√ß√µes entre as vari√°veis num√©ricas para identificar poss√≠veis rela√ß√µes lineares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correla√ß√£o\n",
    "numeric_data = df[numeric_cols]\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "print(\"üîó MATRIZ DE CORRELA√á√ÉO\")\n",
    "print(\"=\" * 30)\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Visualiza√ß√£o da matriz de correla√ß√£o\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=correlation_matrix.values,\n",
    "    x=correlation_matrix.columns,\n",
    "    y=correlation_matrix.columns,\n",
    "    colorscale='RdBu',\n",
    "    zmid=0,\n",
    "    text=correlation_matrix.round(3).values,\n",
    "    texttemplate=\"%{text}\",\n",
    "    textfont={\"size\": 12},\n",
    "    hoverongaps=False\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"üîó Matriz de Correla√ß√£o das Vari√°veis Num√©ricas\",\n",
    "    title_x=0.5,\n",
    "    width=600,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Salvando o gr√°fico\n",
    "fig.write_html(\"/home/ubuntu/matriz_correlacao.html\")\n",
    "print(\"üíæ Gr√°fico salvo como 'matriz_correlacao.html'\")\n",
    "\n",
    "# Interpreta√ß√£o das correla√ß√µes\n",
    "print(\"\\nüìä INTERPRETA√á√ÉO DAS CORRELA√á√ïES:\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        var1 = correlation_matrix.columns[i]\n",
    "        var2 = correlation_matrix.columns[j]\n",
    "        \n",
    "        if abs(corr_value) > 0.7:\n",
    "            strength = \"forte\"\n",
    "        elif abs(corr_value) > 0.3:\n",
    "            strength = \"moderada\"\n",
    "        else:\n",
    "            strength = \"fraca\"\n",
    "        \n",
    "        direction = \"positiva\" if corr_value > 0 else \"negativa\"\n",
    "        print(f\"{var1} vs {var2}: {corr_value:.3f} (correla√ß√£o {strength} {direction})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üîß Pr√©-processamento e Feature Engineering\n",
    "\n",
    "O pr√©-processamento √© uma etapa crucial que pode determinar o sucesso do modelo. Vamos aplicar as seguintes transforma√ß√µes:\n",
    "\n",
    "### üéØ Objetivos do Pr√©-processamento:\n",
    "- **Encoding de vari√°veis categ√≥ricas** (One-Hot Encoding)\n",
    "- **Normaliza√ß√£o de vari√°veis num√©ricas** (StandardScaler)\n",
    "- **Vetoriza√ß√£o de texto** (TF-IDF)\n",
    "- **Cria√ß√£o de novas features**\n",
    "- **Divis√£o treino/teste estratificada**\n",
    "\n",
    "### 4.1 Prepara√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß INICIANDO PR√â-PROCESSAMENTO DOS DADOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Criando uma c√≥pia dos dados para preservar o original\n",
    "df_processed = df.copy()\n",
    "\n",
    "# 1. Feature Engineering - Criando novas vari√°veis\n",
    "print(\"\\nüéØ FEATURE ENGINEERING:\")\n",
    "\n",
    "# Categoria de valor (baixo, m√©dio, alto)\n",
    "df_processed['categoria_valor'] = pd.cut(\n",
    "    df_processed['valor_envolvido'], \n",
    "    bins=3, \n",
    "    labels=['Baixo', 'M√©dio', 'Alto']\n",
    ")\n",
    "\n",
    "# Categoria de tempo de resposta\n",
    "df_processed['categoria_tempo'] = pd.cut(\n",
    "    df_processed['tempo_resposta_dias'], \n",
    "    bins=3, \n",
    "    labels=['R√°pido', 'M√©dio', 'Lento']\n",
    ")\n",
    "\n",
    "# Tamanho do texto da reclama√ß√£o\n",
    "df_processed['tamanho_texto'] = df_processed['texto_reclamacao'].str.len()\n",
    "\n",
    "# N√∫mero de palavras na reclama√ß√£o\n",
    "df_processed['num_palavras'] = df_processed['texto_reclamacao'].str.split().str.len()\n",
    "\n",
    "print(f\"‚úÖ Criadas 4 novas features:\")\n",
    "print(f\"   - categoria_valor: {df_processed['categoria_valor'].nunique()} categorias\")\n",
    "print(f\"   - categoria_tempo: {df_processed['categoria_tempo'].nunique()} categorias\")\n",
    "print(f\"   - tamanho_texto: vari√°vel num√©rica\")\n",
    "print(f\"   - num_palavras: vari√°vel num√©rica\")\n",
    "\n",
    "# Visualizando as novas features\n",
    "print(\"\\nüìä DISTRIBUI√á√ÉO DAS NOVAS FEATURES:\")\n",
    "print(\"\\nCategoria de Valor:\")\n",
    "print(df_processed['categoria_valor'].value_counts())\n",
    "print(\"\\nCategoria de Tempo:\")\n",
    "print(df_processed['categoria_tempo'].value_counts())\n",
    "print(f\"\\nTamanho do texto - M√©dia: {df_processed['tamanho_texto'].mean():.1f} caracteres\")\n",
    "print(f\"N√∫mero de palavras - M√©dia: {df_processed['num_palavras'].mean():.1f} palavras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Divis√£o dos Dados (Train/Test Split)\n",
    "\n",
    "**Boa Pr√°tica**: Sempre dividir os dados ANTES do pr√©-processamento para evitar data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä DIVIS√ÉO DOS DADOS (TRAIN/TEST SPLIT)\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Separando features e target\n",
    "X = df_processed.drop('categoria', axis=1)\n",
    "y = df_processed['categoria']\n",
    "\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")\n",
    "print(f\"Classes: {y.unique()}\")\n",
    "\n",
    "# Divis√£o estratificada (mant√©m a propor√ß√£o das classes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,  # 20% para teste\n",
    "    random_state=42,  # Para reprodutibilidade\n",
    "    stratify=y  # Mant√©m propor√ß√£o das classes\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ DIVIS√ÉO REALIZADA:\")\n",
    "print(f\"Treino: {X_train.shape[0]} amostras ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Teste: {X_test.shape[0]} amostras ({X_test.shape[0]/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Verificando a distribui√ß√£o das classes ap√≥s a divis√£o\n",
    "print(\"\\nüìä DISTRIBUI√á√ÉO DAS CLASSES:\")\n",
    "print(\"\\nTreino:\")\n",
    "train_dist = y_train.value_counts(normalize=True) * 100\n",
    "for classe, perc in train_dist.items():\n",
    "    print(f\"  {classe}: {perc:.1f}%\")\n",
    "\n",
    "print(\"\\nTeste:\")\n",
    "test_dist = y_test.value_counts(normalize=True) * 100\n",
    "for classe, perc in test_dist.items():\n",
    "    print(f\"  {classe}: {perc:.1f}%\")\n",
    "\n",
    "# Verificando se a estratifica√ß√£o funcionou\n",
    "max_diff = max(abs(train_dist - test_dist))\n",
    "if max_diff < 2:\n",
    "    print(f\"\\n‚úÖ Estratifica√ß√£o bem-sucedida! Diferen√ßa m√°xima: {max_diff:.2f}%\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Poss√≠vel problema na estratifica√ß√£o. Diferen√ßa m√°xima: {max_diff:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Pipeline de Pr√©-processamento\n",
    "\n",
    "Vamos criar um pipeline robusto que trata diferentes tipos de vari√°veis de forma adequada:\n",
    "\n",
    "- **Vari√°veis num√©ricas**: StandardScaler\n",
    "- **Vari√°veis categ√≥ricas**: OneHotEncoder\n",
    "- **Texto**: TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîß CRIANDO PIPELINE DE PR√â-PROCESSAMENTO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Definindo as colunas por tipo\n",
    "numeric_features = ['valor_envolvido', 'tempo_resposta_dias', 'tamanho_texto', 'num_palavras']\n",
    "categorical_features = ['empresa', 'estado', 'categoria_valor', 'categoria_tempo']\n",
    "text_feature = 'texto_reclamacao'\n",
    "\n",
    "print(f\"üìä TIPOS DE FEATURES:\")\n",
    "print(f\"Num√©ricas ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Categ√≥ricas ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"Texto (1): {text_feature}\")\n",
    "\n",
    "# Pipeline para vari√°veis num√©ricas\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline para vari√°veis categ√≥ricas\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Pipeline para texto\n",
    "text_transformer = Pipeline(steps=[\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=1000,  # M√°ximo de 1000 features de texto\n",
    "        stop_words=None,  # Sem stop words (portugu√™s n√£o est√° dispon√≠vel)\n",
    "        ngram_range=(1, 2),  # Unigramas e bigramas\n",
    "        min_df=2,  # Palavra deve aparecer em pelo menos 2 documentos\n",
    "        max_df=0.95  # Palavra n√£o pode aparecer em mais de 95% dos documentos\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Combinando todos os transformadores\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('text', text_transformer, text_feature)\n",
    "    ],\n",
    "    remainder='drop'  # Remove colunas n√£o especificadas\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline de pr√©-processamento criado com sucesso!\")\n",
    "print(\"\\nüîß COMPONENTES DO PIPELINE:\")\n",
    "print(\"1. Vari√°veis Num√©ricas ‚Üí StandardScaler\")\n",
    "print(\"2. Vari√°veis Categ√≥ricas ‚Üí OneHotEncoder\")\n",
    "print(\"3. Texto ‚Üí TfidfVectorizer\")\n",
    "print(\"4. Combina√ß√£o ‚Üí ColumnTransformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando o pr√©-processamento\n",
    "print(\"\\n‚öôÔ∏è APLICANDO PR√â-PROCESSAMENTO\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Fit no conjunto de treino e transform em ambos\n",
    "print(\"Ajustando transformadores no conjunto de treino...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "print(\"Aplicando transforma√ß√µes no conjunto de teste...\")\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"\\n‚úÖ PR√â-PROCESSAMENTO CONCLU√çDO:\")\n",
    "print(f\"X_train original: {X_train.shape}\")\n",
    "print(f\"X_train processado: {X_train_processed.shape}\")\n",
    "print(f\"X_test original: {X_test.shape}\")\n",
    "print(f\"X_test processado: {X_test_processed.shape}\")\n",
    "\n",
    "# Informa√ß√µes sobre as features criadas\n",
    "total_features = X_train_processed.shape[1]\n",
    "print(f\"\\nüìä FEATURES FINAIS:\")\n",
    "print(f\"Total de features: {total_features}\")\n",
    "\n",
    "# Estimando a contribui√ß√£o de cada tipo\n",
    "num_numeric = len(numeric_features)\n",
    "# Para categ√≥ricas, precisamos ver quantas foram criadas ap√≥s one-hot\n",
    "cat_encoder = preprocessor.named_transformers_['cat']['onehot']\n",
    "num_categorical = len(cat_encoder.get_feature_names_out())\n",
    "num_text = 1000  # Definido no TfidfVectorizer\n",
    "\n",
    "print(f\"Features num√©ricas: {num_numeric}\")\n",
    "print(f\"Features categ√≥ricas (ap√≥s one-hot): {num_categorical}\")\n",
    "print(f\"Features de texto (TF-IDF): {min(num_text, total_features - num_numeric - num_categorical)}\")\n",
    "\n",
    "# Verificando se h√° valores NaN\n",
    "nan_train = np.isnan(X_train_processed).sum()\n",
    "nan_test = np.isnan(X_test_processed).sum()\n",
    "\n",
    "if nan_train == 0 and nan_test == 0:\n",
    "    print(\"\\n‚úÖ Nenhum valor NaN detectado ap√≥s pr√©-processamento\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Valores NaN detectados - Treino: {nan_train}, Teste: {nan_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Encoding da Vari√°vel Target\n",
    "\n",
    "Vamos codificar a vari√°vel target para uso nos algoritmos de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ ENCODING DA VARI√ÅVEL TARGET\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Criando o encoder para a vari√°vel target\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit no conjunto completo de labels (treino + teste)\n",
    "all_labels = pd.concat([y_train, y_test])\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "# Transform nos conjuntos de treino e teste\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "print(f\"‚úÖ Encoding realizado com sucesso!\")\n",
    "print(f\"\\nüìä MAPEAMENTO DAS CLASSES:\")\n",
    "for i, classe in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {classe} ‚Üí {i}\")\n",
    "\n",
    "print(f\"\\nüìà DISTRIBUI√á√ÉO ENCODED:\")\n",
    "unique, counts = np.unique(y_train_encoded, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    classe_original = label_encoder.inverse_transform([label])[0]\n",
    "    percentage = (count / len(y_train_encoded)) * 100\n",
    "    print(f\"  Classe {label} ({classe_original}): {count} amostras ({percentage:.1f}%)\")\n",
    "\n",
    "# Salvando o label encoder para uso posterior\n",
    "with open('/home/ubuntu/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(\"\\nüíæ Label encoder salvo como 'label_encoder.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ü§ñ Treinamento de Modelos de Machine Learning\n",
    "\n",
    "Agora vamos treinar m√∫ltiplos algoritmos de classifica√ß√£o e comparar seu desempenho. Utilizaremos:\n",
    "\n",
    "### üéØ Algoritmos Selecionados:\n",
    "1. **Random Forest** - Ensemble robusto e interpret√°vel\n",
    "2. **Support Vector Machine (SVM)** - Eficaz para alta dimensionalidade\n",
    "3. **Logistic Regression** - Baseline linear e interpret√°vel\n",
    "4. **Gradient Boosting** - Ensemble sequencial poderoso\n",
    "5. **Naive Bayes** - R√°pido e eficaz para texto\n",
    "\n",
    "### 5.1 Configura√ß√£o dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ CONFIGURA√á√ÉO DOS MODELOS DE MACHINE LEARNING\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Dicion√°rio com os modelos e seus par√¢metros\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    'SVM': SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        gamma='scale',\n",
    "        random_state=42,\n",
    "        probability=True  # Para calcular probabilidades\n",
    "    ),\n",
    "    \n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        multi_class='ovr',\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    'Naive Bayes': MultinomialNB(\n",
    "        alpha=1.0\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"üìä MODELOS CONFIGURADOS: {len(models)}\")\n",
    "for i, (name, model) in enumerate(models.items(), 1):\n",
    "    print(f\"{i}. {name}: {type(model).__name__}\")\n",
    "\n",
    "print(\"\\nüéØ CARACTER√çSTICAS DOS MODELOS:\")\n",
    "print(\"‚Ä¢ Random Forest: Ensemble de √°rvores, robusto a overfitting\")\n",
    "print(\"‚Ä¢ SVM: Eficaz para alta dimensionalidade, kernel RBF\")\n",
    "print(\"‚Ä¢ Logistic Regression: Linear, r√°pido, interpret√°vel\")\n",
    "print(\"‚Ä¢ Gradient Boosting: Ensemble sequencial, alta performance\")\n",
    "print(\"‚Ä¢ Naive Bayes: R√°pido, eficaz para dados de texto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Treinamento e Avalia√ß√£o Inicial\n",
    "\n",
    "Vamos treinar todos os modelos e fazer uma avalia√ß√£o inicial para comparar o desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ INICIANDO TREINAMENTO DOS MODELOS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Dicion√°rio para armazenar resultados\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "# Treinando cada modelo\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîÑ Treinando {name}...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Treinamento\n",
    "    model.fit(X_train_processed, y_train_encoded)\n",
    "    \n",
    "    # Predi√ß√µes\n",
    "    y_train_pred = model.predict(X_train_processed)\n",
    "    y_test_pred = model.predict(X_test_processed)\n",
    "    \n",
    "    # Probabilidades (se dispon√≠vel)\n",
    "    try:\n",
    "        y_test_proba = model.predict_proba(X_test_processed)\n",
    "    except:\n",
    "        y_test_proba = None\n",
    "    \n",
    "    # Calculando m√©tricas\n",
    "    train_accuracy = accuracy_score(y_train_encoded, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test_encoded, y_test_pred)\n",
    "    \n",
    "    # M√©tricas detalhadas para o conjunto de teste\n",
    "    precision = precision_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "    recall = recall_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "    \n",
    "    # Tempo de treinamento\n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Armazenando resultados\n",
    "    results[name] = {\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'training_time': training_time,\n",
    "        'predictions': y_test_pred,\n",
    "        'probabilities': y_test_proba\n",
    "    }\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"   ‚úÖ Conclu√≠do em {training_time:.2f}s\")\n",
    "    print(f\"   üìä Acur√°cia Treino: {train_accuracy:.4f}\")\n",
    "    print(f\"   üìä Acur√°cia Teste: {test_accuracy:.4f}\")\n",
    "    print(f\"   üìä F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nüéâ TREINAMENTO CONCLU√çDO PARA TODOS OS MODELOS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Compara√ß√£o de Resultados\n",
    "\n",
    "Vamos criar uma tabela comparativa com todas as m√©tricas dos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä COMPARA√á√ÉO DE RESULTADOS DOS MODELOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Criando DataFrame com os resultados\n",
    "results_df = pd.DataFrame({\n",
    "    'Modelo': list(results.keys()),\n",
    "    'Acur√°cia Treino': [results[model]['train_accuracy'] for model in results.keys()],\n",
    "    'Acur√°cia Teste': [results[model]['test_accuracy'] for model in results.keys()],\n",
    "    'Precision': [results[model]['precision'] for model in results.keys()],\n",
    "    'Recall': [results[model]['recall'] for model in results.keys()],\n",
    "    'F1-Score': [results[model]['f1_score'] for model in results.keys()],\n",
    "    'Tempo (s)': [results[model]['training_time'] for model in results.keys()]\n",
    "})\n",
    "\n",
    "# Ordenando por F1-Score\n",
    "results_df = results_df.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Formatando para melhor visualiza√ß√£o\n",
    "results_df_display = results_df.copy()\n",
    "for col in ['Acur√°cia Treino', 'Acur√°cia Teste', 'Precision', 'Recall', 'F1-Score']:\n",
    "    results_df_display[col] = results_df_display[col].apply(lambda x: f\"{x:.4f}\")\n",
    "results_df_display['Tempo (s)'] = results_df_display['Tempo (s)'].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "print(\"üèÜ RANKING DOS MODELOS (por F1-Score):\")\n",
    "display(results_df_display)\n",
    "\n",
    "# Identificando o melhor modelo\n",
    "best_model_name = results_df.iloc[0]['Modelo']\n",
    "best_f1 = results_df.iloc[0]['F1-Score']\n",
    "\n",
    "print(f\"\\nü•á MELHOR MODELO: {best_model_name}\")\n",
    "print(f\"   F1-Score: {best_f1:.4f}\")\n",
    "print(f\"   Acur√°cia Teste: {results_df.iloc[0]['Acur√°cia Teste']:.4f}\")\n",
    "\n",
    "# An√°lise de overfitting\n",
    "print(\"\\nüîç AN√ÅLISE DE OVERFITTING:\")\n",
    "for _, row in results_df.iterrows():\n",
    "    modelo = row['Modelo']\n",
    "    diff = row['Acur√°cia Treino'] - row['Acur√°cia Teste']\n",
    "    \n",
    "    if diff < 0.02:\n",
    "        status = \"‚úÖ Bem balanceado\"\n",
    "    elif diff < 0.05:\n",
    "        status = \"‚ö†Ô∏è Leve overfitting\"\n",
    "    else:\n",
    "        status = \"üö® Overfitting detectado\"\n",
    "    \n",
    "    print(f\"  {modelo}: {diff:.4f} - {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Visualiza√ß√£o dos Resultados\n",
    "\n",
    "Vamos criar visualiza√ß√µes para comparar o desempenho dos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o comparativa dos modelos\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        'Compara√ß√£o de Acur√°cia (Treino vs Teste)',\n",
    "        'M√©tricas de Performance',\n",
    "        'Tempo de Treinamento',\n",
    "        'F1-Score por Modelo'\n",
    "    ],\n",
    "    specs=[[{'type': 'scatter'}, {'type': 'bar'}],\n",
    "           [{'type': 'bar'}, {'type': 'bar'}]]\n",
    ")\n",
    "\n",
    "# 1. Scatter plot: Acur√°cia Treino vs Teste\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=results_df['Acur√°cia Treino'],\n",
    "        y=results_df['Acur√°cia Teste'],\n",
    "        mode='markers+text',\n",
    "        text=results_df['Modelo'],\n",
    "        textposition='top center',\n",
    "        marker=dict(size=12, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Linha diagonal (ideal)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0.7, 1.0],\n",
    "        y=[0.7, 1.0],\n",
    "        mode='lines',\n",
    "        line=dict(dash='dash', color='gray'),\n",
    "        name='Linha Ideal',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. M√©tricas de Performance\n",
    "metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=results_df['Modelo'],\n",
    "            y=results_df[metric],\n",
    "            name=metric,\n",
    "            marker_color=colors[i],\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# 3. Tempo de Treinamento\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=results_df['Modelo'],\n",
    "        y=results_df['Tempo (s)'],\n",
    "        text=results_df['Tempo (s)'].apply(lambda x: f\"{x:.2f}s\"),\n",
    "        textposition='auto',\n",
    "        marker_color='#96CEB4',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. F1-Score destacado\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=results_df['Modelo'],\n",
    "        y=results_df['F1-Score'],\n",
    "        text=results_df['F1-Score'].apply(lambda x: f\"{x:.4f}\"),\n",
    "        textposition='auto',\n",
    "        marker_color=['#FFD700' if i == 0 else '#CCCCCC' for i in range(len(results_df))],\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Atualizando layout\n",
    "fig.update_layout(\n",
    "    title_text=\"ü§ñ Compara√ß√£o Completa dos Modelos de Machine Learning\",\n",
    "    title_x=0.5,\n",
    "    height=800,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Atualizando eixos\n",
    "fig.update_xaxes(title_text=\"Acur√°cia Treino\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Acur√°cia Teste\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Score\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Tempo (segundos)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"F1-Score\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Salvando o gr√°fico\n",
    "fig.write_html(\"/home/ubuntu/comparacao_modelos.html\")\n",
    "print(\"üíæ Gr√°fico salvo como 'comparacao_modelos.html'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üîç Valida√ß√£o Cruzada e Ajuste de Hiperpar√¢metros\n",
    "\n",
    "Vamos aplicar **valida√ß√£o cruzada** para obter uma avalia√ß√£o mais robusta e realizar **ajuste de hiperpar√¢metros** no melhor modelo.\n",
    "\n",
    "### 6.1 Valida√ß√£o Cruzada Estratificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç VALIDA√á√ÉO CRUZADA ESTRATIFICADA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Configurando valida√ß√£o cruzada estratificada\n",
    "cv_folds = 5\n",
    "cv_strategy = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"üìä Configura√ß√£o: {cv_folds} folds estratificados\")\n",
    "print(f\"üéØ M√©trica: F1-Score (weighted)\")\n",
    "\n",
    "# Resultados da valida√ß√£o cruzada\n",
    "cv_results = {}\n",
    "\n",
    "print(\"\\nüîÑ Executando valida√ß√£o cruzada para todos os modelos...\")\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"\\nüìä {name}:\")\n",
    "    \n",
    "    # Valida√ß√£o cruzada com F1-Score\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_processed, y_train_encoded,\n",
    "        cv=cv_strategy,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    mean_score = cv_scores.mean()\n",
    "    std_score = cv_scores.std()\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'scores': cv_scores,\n",
    "        'mean': mean_score,\n",
    "        'std': std_score,\n",
    "        'min': cv_scores.min(),\n",
    "        'max': cv_scores.max()\n",
    "    }\n",
    "    \n",
    "    print(f\"   M√©dia: {mean_score:.4f} (¬±{std_score:.4f})\")\n",
    "    print(f\"   Min: {cv_scores.min():.4f} | Max: {cv_scores.max():.4f}\")\n",
    "    print(f\"   Scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
    "\n",
    "# Criando DataFrame com resultados da CV\n",
    "cv_df = pd.DataFrame({\n",
    "    'Modelo': list(cv_results.keys()),\n",
    "    'CV_Mean': [cv_results[model]['mean'] for model in cv_results.keys()],\n",
    "    'CV_Std': [cv_results[model]['std'] for model in cv_results.keys()],\n",
    "    'CV_Min': [cv_results[model]['min'] for model in cv_results.keys()],\n",
    "    'CV_Max': [cv_results[model]['max'] for model in cv_results.keys()]\n",
    "})\n",
    "\n",
    "# Ordenando por m√©dia da CV\n",
    "cv_df = cv_df.sort_values('CV_Mean', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nüèÜ RANKING POR VALIDA√á√ÉO CRUZADA:\")\n",
    "display(cv_df.round(4))\n",
    "\n",
    "# Melhor modelo por CV\n",
    "best_cv_model = cv_df.iloc[0]['Modelo']\n",
    "best_cv_score = cv_df.iloc[0]['CV_Mean']\n",
    "best_cv_std = cv_df.iloc[0]['CV_Std']\n",
    "\n",
    "print(f\"\\nü•á MELHOR MODELO (Valida√ß√£o Cruzada): {best_cv_model}\")\n",
    "print(f\"   F1-Score CV: {best_cv_score:.4f} (¬±{best_cv_std:.4f})\")\n",
    "\n",
    "# Verificando consist√™ncia\n",
    "if best_cv_model == best_model_name:\n",
    "    print(\"\\n‚úÖ Consist√™ncia confirmada! Mesmo melhor modelo em ambas avalia√ß√µes.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Diverg√™ncia detectada! Melhor por holdout: {best_model_name}, por CV: {best_cv_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o dos resultados da valida√ß√£o cruzada\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['Distribui√ß√£o dos Scores de Valida√ß√£o Cruzada', 'Compara√ß√£o: Holdout vs Cross-Validation'],\n",
    "    specs=[[{'type': 'box'}, {'type': 'scatter'}]]\n",
    ")\n",
    "\n",
    "# 1. Boxplot dos scores de CV\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']\n",
    "for i, (name, data) in enumerate(cv_results.items()):\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=data['scores'],\n",
    "            name=name,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# 2. Scatter: Holdout vs CV\n",
    "holdout_scores = [results[model]['f1_score'] for model in cv_df['Modelo']]\n",
    "cv_scores_mean = cv_df['CV_Mean'].tolist()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=holdout_scores,\n",
    "        y=cv_scores_mean,\n",
    "        mode='markers+text',\n",
    "        text=cv_df['Modelo'],\n",
    "        textposition='top center',\n",
    "        marker=dict(size=12, color=colors[:len(cv_df)]),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Linha diagonal\n",
    "min_score = min(min(holdout_scores), min(cv_scores_mean))\n",
    "max_score = max(max(holdout_scores), max(cv_scores_mean))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[min_score, max_score],\n",
    "        y=[min_score, max_score],\n",
    "        mode='lines',\n",
    "        line=dict(dash='dash', color='gray'),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"üîç An√°lise de Valida√ß√£o Cruzada\",\n",
    "    title_x=0.5,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text=\"F1-Score\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"F1-Score (Holdout)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"F1-Score (Cross-Validation)\", row=1, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Salvando o gr√°fico\n",
    "fig.write_html(\"/home/ubuntu/validacao_cruzada.html\")\n",
    "print(\"üíæ Gr√°fico salvo como 'validacao_cruzada.html'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Ajuste de Hiperpar√¢metros (Grid Search)\n",
    "\n",
    "Vamos otimizar os hiperpar√¢metros do melhor modelo usando Grid Search com valida√ß√£o cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è AJUSTE DE HIPERPAR√ÇMETROS (GRID SEARCH)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Selecionando o melhor modelo para otimiza√ß√£o\n",
    "model_to_optimize = best_cv_model\n",
    "base_model = trained_models[model_to_optimize]\n",
    "\n",
    "print(f\"üéØ Modelo selecionado: {model_to_optimize}\")\n",
    "print(f\"üìä Score atual: {best_cv_score:.4f} (¬±{best_cv_std:.4f})\")\n",
    "\n",
    "# Definindo grid de hiperpar√¢metros baseado no modelo\n",
    "if 'Random Forest' in model_to_optimize:\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 15, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "elif 'SVM' in model_to_optimize:\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "    \n",
    "elif 'Logistic' in model_to_optimize:\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['liblinear', 'lbfgs'],\n",
    "        'max_iter': [500, 1000, 2000]\n",
    "    }\n",
    "    \n",
    "elif 'Gradient' in model_to_optimize:\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    \n",
    "else:  # Naive Bayes\n",
    "    param_grid = {\n",
    "        'alpha': [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "    }\n",
    "\n",
    "print(f\"\\nüîß Grid de par√¢metros definido:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"   {param}: {values}\")\n",
    "\n",
    "total_combinations = 1\n",
    "for values in param_grid.values():\n",
    "    total_combinations *= len(values)\n",
    "print(f\"\\nüìä Total de combina√ß√µes: {total_combinations}\")\n",
    "print(f\"üìä Total de fits: {total_combinations * cv_folds} (com {cv_folds}-fold CV)\")\n",
    "\n",
    "# Executando Grid Search\n",
    "print(\"\\nüöÄ Iniciando Grid Search...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Criando uma nova inst√¢ncia do modelo\n",
    "if 'Random Forest' in model_to_optimize:\n",
    "    model_for_grid = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "elif 'SVM' in model_to_optimize:\n",
    "    model_for_grid = SVC(random_state=42, probability=True)\n",
    "elif 'Logistic' in model_to_optimize:\n",
    "    model_for_grid = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "elif 'Gradient' in model_to_optimize:\n",
    "    model_for_grid = GradientBoostingClassifier(random_state=42)\n",
    "else:\n",
    "    model_for_grid = MultinomialNB()\n",
    "\n",
    "# Grid Search com valida√ß√£o cruzada\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model_for_grid,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv_strategy,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_processed, y_train_encoded)\n",
    "\n",
    "optimization_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\n‚úÖ Grid Search conclu√≠do em {optimization_time:.2f}s\")\n",
    "print(f\"\\nüèÜ MELHORES HIPERPAR√ÇMETROS:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä MELHORES RESULTADOS:\")\n",
    "print(f\"   F1-Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"   Melhoria: {grid_search.best_score_ - best_cv_score:.4f}\")\n",
    "\n",
    "# Modelo otimizado\n",
    "optimized_model = grid_search.best_estimator_\n",
    "\n",
    "# Avalia√ß√£o no conjunto de teste\n",
    "y_test_pred_optimized = optimized_model.predict(X_test_processed)\n",
    "test_f1_optimized = f1_score(y_test_encoded, y_test_pred_optimized, average='weighted')\n",
    "test_accuracy_optimized = accuracy_score(y_test_encoded, y_test_pred_optimized)\n",
    "\n",
    "print(f\"\\nüéØ PERFORMANCE NO CONJUNTO DE TESTE:\")\n",
    "print(f\"   Acur√°cia: {test_accuracy_optimized:.4f}\")\n",
    "print(f\"   F1-Score: {test_f1_optimized:.4f}\")\n",
    "print(f\"   Melhoria na acur√°cia: {test_accuracy_optimized - results[model_to_optimize]['test_accuracy']:.4f}\")\n",
    "print(f\"   Melhoria no F1: {test_f1_optimized - results[model_to_optimize]['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üìä Avalia√ß√£o Detalhada do Melhor Modelo\n",
    "\n",
    "Vamos fazer uma an√°lise detalhada do modelo otimizado, incluindo:\n",
    "- **Matriz de Confus√£o**\n",
    "- **Relat√≥rio de Classifica√ß√£o**\n",
    "- **An√°lise por Classe**\n",
    "- **Import√¢ncia das Features** (se aplic√°vel)\n",
    "\n",
    "### 7.1 Matriz de Confus√£o e M√©tricas Detalhadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä AVALIA√á√ÉO DETALHADA DO MODELO OTIMIZADO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Predi√ß√µes do modelo otimizado\n",
    "y_test_pred_final = optimized_model.predict(X_test_processed)\n",
    "y_test_proba_final = optimized_model.predict_proba(X_test_processed)\n",
    "\n",
    "# Matriz de confus√£o\n",
    "cm = confusion_matrix(y_test_encoded, y_test_pred_final)\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "print(f\"üéØ MODELO FINAL: {model_to_optimize} (Otimizado)\")\n",
    "print(f\"üìä Acur√°cia: {test_accuracy_optimized:.4f}\")\n",
    "print(f\"üìä F1-Score: {test_f1_optimized:.4f}\")\n",
    "\n",
    "# Relat√≥rio de classifica√ß√£o detalhado\n",
    "print(\"\\nüìã RELAT√ìRIO DE CLASSIFICA√á√ÉO:\")\n",
    "classification_rep = classification_report(\n",
    "    y_test_encoded, y_test_pred_final,\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Convertendo para DataFrame para melhor visualiza√ß√£o\n",
    "report_df = pd.DataFrame(classification_rep).transpose()\n",
    "print(report_df.round(4))\n",
    "\n",
    "# Visualiza√ß√£o da matriz de confus√£o\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['Matriz de Confus√£o (Valores Absolutos)', 'Matriz de Confus√£o (Normalizada)'],\n",
    "    specs=[[{'type': 'heatmap'}, {'type': 'heatmap'}]]\n",
    ")\n",
    "\n",
    "# Matriz de confus√£o absoluta\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=cm,\n",
    "        x=class_names,\n",
    "        y=class_names,\n",
    "        colorscale='Blues',\n",
    "        text=cm,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 12},\n",
    "        showscale=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Matriz de confus√£o normalizada\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=cm_normalized,\n",
    "        x=class_names,\n",
    "        y=class_names,\n",
    "        colorscale='Reds',\n",
    "        text=np.round(cm_normalized, 3),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 12},\n",
    "        showscale=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=f\"üìä Matriz de Confus√£o - {model_to_optimize} (Otimizado)\",\n",
    "    title_x=0.5,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Predito\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Real\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Predito\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Real\", row=1, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Salvando o gr√°fico\n",
    "fig.write_html(\"/home/ubuntu/matriz_confusao_final.html\")\n",
    "print(\"üíæ Gr√°fico salvo como 'matriz_confusao_final.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise detalhada por classe\n",
    "print(\"\\nüîç AN√ÅLISE DETALHADA POR CLASSE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    # M√©tricas da classe\n",
    "    precision = classification_rep[class_name]['precision']\n",
    "    recall = classification_rep[class_name]['recall']\n",
    "    f1 = classification_rep[class_name]['f1-score']\n",
    "    support = int(classification_rep[class_name]['support'])\n",
    "    \n",
    "    # An√°lise da matriz de confus√£o para esta classe\n",
    "    true_positives = cm[i, i]\n",
    "    false_positives = cm[:, i].sum() - true_positives\n",
    "    false_negatives = cm[i, :].sum() - true_positives\n",
    "    true_negatives = cm.sum() - true_positives - false_positives - false_negatives\n",
    "    \n",
    "    print(f\"\\nüìä {class_name.upper()}:\")\n",
    "    print(f\"   Precision: {precision:.4f} | Recall: {recall:.4f} | F1-Score: {f1:.4f}\")\n",
    "    print(f\"   Support: {support} amostras\")\n",
    "    print(f\"   TP: {true_positives} | FP: {false_positives} | FN: {false_negatives} | TN: {true_negatives}\")\n",
    "    \n",
    "    # Interpreta√ß√£o\n",
    "    if precision >= 0.9 and recall >= 0.9:\n",
    "        interpretation = \"‚úÖ Excelente performance\"\n",
    "    elif precision >= 0.8 and recall >= 0.8:\n",
    "        interpretation = \"‚úÖ Boa performance\"\n",
    "    elif precision >= 0.7 or recall >= 0.7:\n",
    "        interpretation = \"‚ö†Ô∏è Performance moderada\"\n",
    "    else:\n",
    "        interpretation = \"üö® Performance baixa\"\n",
    "    \n",
    "    print(f\"   {interpretation}\")\n",
    "    \n",
    "    # An√°lise espec√≠fica\n",
    "    if precision > recall:\n",
    "        print(f\"   üí° Modelo conservador: poucos falsos positivos, mas pode perder alguns casos\")\n",
    "    elif recall > precision:\n",
    "        print(f\"   üí° Modelo sens√≠vel: captura bem os casos, mas com alguns falsos positivos\")\n",
    "    else:\n",
    "        print(f\"   üí° Modelo balanceado: precision e recall equilibrados\")\n",
    "\n",
    "# M√©tricas globais\n",
    "macro_avg = classification_rep['macro avg']\n",
    "weighted_avg = classification_rep['weighted avg']\n",
    "\n",
    "print(f\"\\nüåç M√âTRICAS GLOBAIS:\")\n",
    "print(f\"Macro Average - Precision: {macro_avg['precision']:.4f} | Recall: {macro_avg['recall']:.4f} | F1: {macro_avg['f1-score']:.4f}\")\n",
    "print(f\"Weighted Average - Precision: {weighted_avg['precision']:.4f} | Recall: {weighted_avg['recall']:.4f} | F1: {weighted_avg['f1-score']:.4f}\")\n",
    "print(f\"Accuracy: {classification_rep['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 An√°lise de Import√¢ncia das Features\n",
    "\n",
    "Para modelos que suportam, vamos analisar quais features s√£o mais importantes para a classifica√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç AN√ÅLISE DE IMPORT√ÇNCIA DAS FEATURES\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Verificando se o modelo suporta feature importance\n",
    "if hasattr(optimized_model, 'feature_importances_'):\n",
    "    # Obtendo import√¢ncias\n",
    "    feature_importances = optimized_model.feature_importances_\n",
    "    \n",
    "    # Obtendo nomes das features do preprocessor\n",
    "    feature_names = []\n",
    "    \n",
    "    # Features num√©ricas\n",
    "    feature_names.extend(numeric_features)\n",
    "    \n",
    "    # Features categ√≥ricas (ap√≥s one-hot encoding)\n",
    "    cat_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
    "    feature_names.extend(cat_feature_names)\n",
    "    \n",
    "    # Features de texto (TF-IDF)\n",
    "    text_feature_names = preprocessor.named_transformers_['text']['tfidf'].get_feature_names_out()\n",
    "    feature_names.extend([f\"text_{name}\" for name in text_feature_names])\n",
    "    \n",
    "    # Criando DataFrame com import√¢ncias\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"‚úÖ An√°lise de import√¢ncia dispon√≠vel para {model_to_optimize}\")\n",
    "    print(f\"üìä Total de features: {len(feature_names)}\")\n",
    "    \n",
    "    # Top 20 features mais importantes\n",
    "    top_features = importance_df.head(20)\n",
    "    print(\"\\nüèÜ TOP 20 FEATURES MAIS IMPORTANTES:\")\n",
    "    for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "        print(f\"{i:2d}. {row['feature'][:50]:<50} {row['importance']:.6f}\")\n",
    "    \n",
    "    # An√°lise por tipo de feature\n",
    "    print(\"\\nüìä IMPORT√ÇNCIA POR TIPO DE FEATURE:\")\n",
    "    \n",
    "    # Num√©ricas\n",
    "    numeric_importance = importance_df[importance_df['feature'].isin(numeric_features)]['importance'].sum()\n",
    "    print(f\"Features Num√©ricas: {numeric_importance:.4f} ({numeric_importance*100:.1f}%)\")\n",
    "    \n",
    "    # Categ√≥ricas\n",
    "    categorical_importance = importance_df[importance_df['feature'].isin(cat_feature_names)]['importance'].sum()\n",
    "    print(f\"Features Categ√≥ricas: {categorical_importance:.4f} ({categorical_importance*100:.1f}%)\")\n",
    "    \n",
    "    # Texto\n",
    "    text_importance = importance_df[importance_df['feature'].str.startswith('text_')]['importance'].sum()\n",
    "    print(f\"Features de Texto: {text_importance:.4f} ({text_importance*100:.1f}%)\")\n",
    "    \n",
    "    # Visualiza√ß√£o das top features\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=top_features['importance'],\n",
    "        y=top_features['feature'],\n",
    "        orientation='h',\n",
    "        marker_color='#4ECDC4',\n",
    "        text=top_features['importance'].round(4),\n",
    "        textposition='auto'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"üîç Top 20 Features Mais Importantes - {model_to_optimize}\",\n",
    "        title_x=0.5,\n",
    "        xaxis_title=\"Import√¢ncia\",\n",
    "        yaxis_title=\"Features\",\n",
    "        height=800,\n",
    "        yaxis={'categoryorder': 'total ascending'}\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Salvando o gr√°fico\n",
    "    fig.write_html(\"/home/ubuntu/feature_importance.html\")\n",
    "    print(\"üíæ Gr√°fico salvo como 'feature_importance.html'\")\n",
    "    \n",
    "    # Salvando import√¢ncias\n",
    "    importance_df.to_csv('/home/ubuntu/feature_importance.csv', index=False)\n",
    "    print(\"üíæ Import√¢ncias salvas como 'feature_importance.csv'\")\n",
    "    \n",
    "elif hasattr(optimized_model, 'coef_'):\n",
    "    print(f\"‚úÖ Modelo {model_to_optimize} possui coeficientes (modelo linear)\")\n",
    "    print(\"üí° Para an√°lise detalhada de coeficientes, seria necess√°rio implementa√ß√£o espec√≠fica\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è Modelo {model_to_optimize} n√£o suporta an√°lise de import√¢ncia de features\")\n",
    "    print(\"üí° Considere usar modelos como Random Forest ou Gradient Boosting para esta an√°lise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üîÆ Predi√ß√£o em Novos Dados\n",
    "\n",
    "Vamos demonstrar como usar o modelo treinado para fazer predi√ß√µes em novos dados, simulando um cen√°rio real de uso.\n",
    "\n",
    "### 8.1 Cria√ß√£o de Dados de Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÆ PREDI√á√ÉO EM NOVOS DADOS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Criando exemplos de novas reclama√ß√µes para predi√ß√£o\n",
    "novos_dados = [\n",
    "    {\n",
    "        'texto_reclamacao': 'Comprei um celular que chegou com a tela quebrada e n√£o funciona',\n",
    "        'empresa': 'TechMart',\n",
    "        'estado': 'SP',\n",
    "        'valor_envolvido': 800.0,\n",
    "        'tempo_resposta_dias': 3.0\n",
    "    },\n",
    "    {\n",
    "        'texto_reclamacao': 'Fui muito mal atendido na loja, funcion√°rio foi grosseiro',\n",
    "        'empresa': 'SuperCompras',\n",
    "        'estado': 'RJ',\n",
    "        'valor_envolvido': 0.0,\n",
    "        'tempo_resposta_dias': 7.0\n",
    "    },\n",
    "    {\n",
    "        'texto_reclamacao': 'Cobraram taxa que n√£o foi informada na contrata√ß√£o do servi√ßo',\n",
    "        'empresa': 'FastDelivery',\n",
    "        'estado': 'MG',\n",
    "        'valor_envolvido': 45.90,\n",
    "        'tempo_resposta_dias': 2.0\n",
    "    },\n",
    "    {\n",
    "        'texto_reclamacao': 'Produto n√£o chegou no prazo prometido, j√° passou 15 dias',\n",
    "        'empresa': 'ExpressShop',\n",
    "        'estado': 'RS',\n",
    "        'valor_envolvido': 150.0,\n",
    "        'tempo_resposta_dias': 12.0\n",
    "    },\n",
    "    {\n",
    "        'texto_reclamacao': 'Aparelho parou de funcionar ap√≥s 2 dias de uso, defeito de f√°brica',\n",
    "        'empresa': 'EletroMax',\n",
    "        'estado': 'PR',\n",
    "        'valor_envolvido': 1200.0,\n",
    "        'tempo_resposta_dias': 5.0\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convertendo para DataFrame\n",
    "df_novos = pd.DataFrame(novos_dados)\n",
    "\n",
    "print(f\"üìä Criados {len(novos_dados)} exemplos de novas reclama√ß√µes\")\n",
    "print(\"\\nüìã NOVOS DADOS PARA PREDI√á√ÉO:\")\n",
    "for i, row in df_novos.iterrows():\n",
    "    print(f\"\\n{i+1}. Empresa: {row['empresa']} | Estado: {row['estado']} | Valor: R$ {row['valor_envolvido']}\")\n",
    "    print(f\"   Reclama√ß√£o: {row['texto_reclamacao']}\")\n",
    "    print(f\"   Tempo resposta: {row['tempo_resposta_dias']} dias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Pr√©-processamento dos Novos Dados\n",
    "\n",
    "**Importante**: Devemos aplicar exatamente as mesmas transforma√ß√µes que foram usadas no treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîß PR√â-PROCESSAMENTO DOS NOVOS DADOS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Aplicando feature engineering nos novos dados\n",
    "df_novos_processed = df_novos.copy()\n",
    "\n",
    "# Criando as mesmas features que foram criadas no treinamento\n",
    "print(\"üéØ Aplicando Feature Engineering...\")\n",
    "\n",
    "# Categoria de valor\n",
    "# Precisamos usar os mesmos bins que foram criados no treinamento\n",
    "valor_bins = pd.qcut(df['valor_envolvido'], q=3, retbins=True)[1]\n",
    "df_novos_processed['categoria_valor'] = pd.cut(\n",
    "    df_novos_processed['valor_envolvido'], \n",
    "    bins=valor_bins, \n",
    "    labels=['Baixo', 'M√©dio', 'Alto'],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Categoria de tempo\n",
    "tempo_bins = pd.qcut(df['tempo_resposta_dias'], q=3, retbins=True)[1]\n",
    "df_novos_processed['categoria_tempo'] = pd.cut(\n",
    "    df_novos_processed['tempo_resposta_dias'], \n",
    "    bins=tempo_bins, \n",
    "    labels=['R√°pido', 'M√©dio', 'Lento'],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Tamanho do texto\n",
    "df_novos_processed['tamanho_texto'] = df_novos_processed['texto_reclamacao'].str.len()\n",
    "\n",
    "# N√∫mero de palavras\n",
    "df_novos_processed['num_palavras'] = df_novos_processed['texto_reclamacao'].str.split().str.len()\n",
    "\n",
    "print(\"‚úÖ Feature Engineering aplicado\")\n",
    "\n",
    "# Aplicando o mesmo pr√©-processamento usado no treinamento\n",
    "print(\"\\n‚öôÔ∏è Aplicando transforma√ß√µes...\")\n",
    "X_novos_processed = preprocessor.transform(df_novos_processed)\n",
    "\n",
    "print(f\"‚úÖ Pr√©-processamento conclu√≠do\")\n",
    "print(f\"üìä Shape dos dados processados: {X_novos_processed.shape}\")\n",
    "print(f\"üìä Mesma dimensionalidade do treino: {X_novos_processed.shape[1] == X_train_processed.shape[1]}\")\n",
    "\n",
    "# Verificando se h√° problemas\n",
    "if np.isnan(X_novos_processed).any():\n",
    "    print(\"‚ö†Ô∏è Aten√ß√£o: Valores NaN detectados nos dados processados\")\n",
    "else:\n",
    "    print(\"‚úÖ Nenhum valor NaN detectado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Realizando Predi√ß√µes\n",
    "\n",
    "Agora vamos usar o modelo otimizado para fazer predi√ß√µes nos novos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÆ REALIZANDO PREDI√á√ïES\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Fazendo predi√ß√µes\n",
    "predicoes = optimized_model.predict(X_novos_processed)\n",
    "probabilidades = optimized_model.predict_proba(X_novos_processed)\n",
    "\n",
    "# Convertendo predi√ß√µes num√©ricas para labels originais\n",
    "predicoes_labels = label_encoder.inverse_transform(predicoes)\n",
    "\n",
    "print(f\"‚úÖ Predi√ß√µes realizadas com sucesso!\")\n",
    "print(f\"üìä {len(predicoes)} predi√ß√µes geradas\")\n",
    "\n",
    "# Criando DataFrame com resultados\n",
    "resultados = df_novos.copy()\n",
    "resultados['categoria_predita'] = predicoes_labels\n",
    "resultados['confianca_max'] = probabilidades.max(axis=1)\n",
    "\n",
    "# Adicionando probabilidades para cada classe\n",
    "for i, classe in enumerate(label_encoder.classes_):\n",
    "    resultados[f'prob_{classe}'] = probabilidades[:, i]\n",
    "\n",
    "print(\"\\nüéØ RESULTADOS DAS PREDI√á√ïES:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i, row in resultados.iterrows():\n",
    "    print(f\"\\nüìã RECLAMA√á√ÉO {i+1}:\")\n",
    "    print(f\"   Texto: {row['texto_reclamacao'][:80]}...\")\n",
    "    print(f\"   Empresa: {row['empresa']} | Estado: {row['estado']} | Valor: R$ {row['valor_envolvido']}\")\n",
    "    print(f\"   \\nüéØ PREDI√á√ÉO: {row['categoria_predita']}\")\n",
    "    print(f\"   üé≤ Confian√ßa: {row['confianca_max']:.2%}\")\n",
    "    \n",
    "    # Mostrando probabilidades de todas as classes\n",
    "    print(f\"   üìä Probabilidades:\")\n",
    "    for classe in label_encoder.classes_:\n",
    "        prob = row[f'prob_{classe}']\n",
    "        bar = \"‚ñà\" * int(prob * 20)  # Barra visual\n",
    "        print(f\"      {classe:<20} {prob:.2%} {bar}\")\n",
    "    \n",
    "    # Interpreta√ß√£o da confian√ßa\n",
    "    if row['confianca_max'] >= 0.8:\n",
    "        confianca_status = \"üü¢ Alta confian√ßa\"\n",
    "    elif row['confianca_max'] >= 0.6:\n",
    "        confianca_status = \"üü° Confian√ßa moderada\"\n",
    "    else:\n",
    "        confianca_status = \"üî¥ Baixa confian√ßa - revisar manualmente\"\n",
    "    \n",
    "    print(f\"   {confianca_status}\")\n",
    "\n",
    "# Salvando resultados\n",
    "resultados.to_csv('/home/ubuntu/predicoes_novos_dados.csv', index=False)\n",
    "print(\"\\nüíæ Resultados salvos como 'predicoes_novos_dados.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Visualiza√ß√£o das Predi√ß√µes\n",
    "\n",
    "Vamos criar visualiza√ß√µes para melhor entender as predi√ß√µes realizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o das predi√ß√µes\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        'Distribui√ß√£o das Predi√ß√µes',\n",
    "        'Confian√ßa das Predi√ß√µes',\n",
    "        'Probabilidades por Reclama√ß√£o',\n",
    "        'An√°lise de Confian√ßa por Categoria'\n",
    "    ],\n",
    "    specs=[[{'type': 'pie'}, {'type': 'bar'}],\n",
    "           [{'type': 'bar'}, {'type': 'box'}]]\n",
    ")\n",
    "\n",
    "# 1. Distribui√ß√£o das predi√ß√µes\n",
    "pred_counts = pd.Series(predicoes_labels).value_counts()\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=pred_counts.index,\n",
    "        values=pred_counts.values,\n",
    "        textinfo='label+percent+value',\n",
    "        marker_colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Confian√ßa das predi√ß√µes\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=[f\"Reclama√ß√£o {i+1}\" for i in range(len(resultados))],\n",
    "        y=resultados['confianca_max'],\n",
    "        text=resultados['confianca_max'].apply(lambda x: f\"{x:.1%}\"),\n",
    "        textposition='auto',\n",
    "        marker_color=['#4ECDC4' if x >= 0.8 else '#FECA57' if x >= 0.6 else '#FF6B6B' \n",
    "                     for x in resultados['confianca_max']],\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Heatmap de probabilidades\n",
    "prob_matrix = []\n",
    "for i, row in resultados.iterrows():\n",
    "    prob_row = [row[f'prob_{classe}'] for classe in label_encoder.classes_]\n",
    "    prob_matrix.append(prob_row)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=prob_matrix,\n",
    "        x=label_encoder.classes_,\n",
    "        y=[f\"Rec. {i+1}\" for i in range(len(resultados))],\n",
    "        colorscale='Viridis',\n",
    "        text=np.round(prob_matrix, 3),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 10},\n",
    "        showscale=False\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Box plot de confian√ßa por categoria predita\n",
    "for categoria in pred_counts.index:\n",
    "    mask = resultados['categoria_predita'] == categoria\n",
    "    confiancas = resultados[mask]['confianca_max']\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=confiancas,\n",
    "            name=categoria,\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"üîÆ An√°lise das Predi√ß√µes em Novos Dados\",\n",
    "    title_x=0.5,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Adicionando linha de refer√™ncia para confian√ßa\n",
    "fig.add_hline(y=0.8, line_dash=\"dash\", line_color=\"green\", \n",
    "              annotation_text=\"Alta Confian√ßa\", row=1, col=2)\n",
    "fig.add_hline(y=0.6, line_dash=\"dash\", line_color=\"orange\", \n",
    "              annotation_text=\"Confian√ßa Moderada\", row=1, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Confian√ßa\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Confian√ßa\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Salvando o gr√°fico\n",
    "fig.write_html(\"/home/ubuntu/analise_predicoes.html\")\n",
    "print(\"üíæ Gr√°fico salvo como 'analise_predicoes.html'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üíæ Salvamento de Modelos e Transformadores\n",
    "\n",
    "Vamos salvar todos os componentes necess√°rios para usar o modelo em produ√ß√£o:\n",
    "- **Modelo treinado otimizado**\n",
    "- **Preprocessor (pipeline de transforma√ß√µes)**\n",
    "- **Label encoder**\n",
    "- **Metadados do modelo**\n",
    "\n",
    "### 9.1 Salvamento dos Componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ SALVAMENTO DE MODELOS E TRANSFORMADORES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Criando diret√≥rio para os modelos\n",
    "import os\n",
    "model_dir = '/home/ubuntu/modelos_treinados'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Diret√≥rio criado: {model_dir}\")\n",
    "\n",
    "# 1. Salvando o modelo otimizado\n",
    "model_path = os.path.join(model_dir, 'modelo_classificacao_reclamacoes.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(optimized_model, f)\n",
    "print(f\"‚úÖ Modelo salvo: {model_path}\")\n",
    "\n",
    "# 2. Salvando o preprocessor\n",
    "preprocessor_path = os.path.join(model_dir, 'preprocessor.pkl')\n",
    "with open(preprocessor_path, 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "print(f\"‚úÖ Preprocessor salvo: {preprocessor_path}\")\n",
    "\n",
    "# 3. Salvando o label encoder\n",
    "label_encoder_path = os.path.join(model_dir, 'label_encoder.pkl')\n",
    "with open(label_encoder_path, 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(f\"‚úÖ Label encoder salvo: {label_encoder_path}\")\n",
    "\n",
    "# 4. Salvando metadados do modelo\n",
    "metadata = {\n",
    "    'modelo_tipo': model_to_optimize,\n",
    "    'data_treinamento': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'versao_sklearn': '1.3.0',  # Vers√£o aproximada\n",
    "    'acuracia_teste': test_accuracy_optimized,\n",
    "    'f1_score_teste': test_f1_optimized,\n",
    "    'cv_score_medio': grid_search.best_score_,\n",
    "    'melhores_parametros': grid_search.best_params_,\n",
    "    'classes': label_encoder.classes_.tolist(),\n",
    "    'features_numericas': numeric_features,\n",
    "    'features_categoricas': categorical_features,\n",
    "    'feature_texto': text_feature,\n",
    "    'total_features': X_train_processed.shape[1],\n",
    "    'total_amostras_treino': X_train_processed.shape[0],\n",
    "    'total_amostras_teste': X_test_processed.shape[0]\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(model_dir, 'metadata.json')\n",
    "import json\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "print(f\"‚úÖ Metadados salvos: {metadata_path}\")\n",
    "\n",
    "# 5. Salvando bins para feature engineering\n",
    "bins_data = {\n",
    "    'valor_bins': valor_bins.tolist(),\n",
    "    'tempo_bins': tempo_bins.tolist()\n",
    "}\n",
    "\n",
    "bins_path = os.path.join(model_dir, 'feature_bins.json')\n",
    "with open(bins_path, 'w') as f:\n",
    "    json.dump(bins_data, f, indent=2)\n",
    "print(f\"‚úÖ Bins para feature engineering salvos: {bins_path}\")\n",
    "\n",
    "print(f\"\\nüéâ SALVAMENTO CONCLU√çDO!\")\n",
    "print(f\"üìä Total de arquivos salvos: 5\")\n",
    "print(f\"üìÅ Localiza√ß√£o: {model_dir}\")\n",
    "\n",
    "# Listando arquivos salvos\n",
    "print(f\"\\nüìã ARQUIVOS SALVOS:\")\n",
    "for arquivo in os.listdir(model_dir):\n",
    "    caminho_completo = os.path.join(model_dir, arquivo)\n",
    "    tamanho = os.path.getsize(caminho_completo) / 1024  # KB\n",
    "    print(f\"   üìÑ {arquivo} ({tamanho:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Fun√ß√£o para Carregar e Usar o Modelo\n",
    "\n",
    "Vamos criar uma fun√ß√£o que demonstra como carregar e usar o modelo salvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîß CRIANDO FUN√á√ÉO PARA USO DO MODELO EM PRODU√á√ÉO\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Fun√ß√£o para carregar o modelo completo\n",
    "def carregar_modelo_completo(diretorio_modelo):\n",
    "    \"\"\"\n",
    "    Carrega todos os componentes necess√°rios para fazer predi√ß√µes.\n",
    "    \n",
    "    Args:\n",
    "        diretorio_modelo (str): Caminho para o diret√≥rio com os modelos salvos\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dicion√°rio com todos os componentes carregados\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    componentes = {}\n",
    "    \n",
    "    # Carregando modelo\n",
    "    with open(os.path.join(diretorio_modelo, 'modelo_classificacao_reclamacoes.pkl'), 'rb') as f:\n",
    "        componentes['modelo'] = pickle.load(f)\n",
    "    \n",
    "    # Carregando preprocessor\n",
    "    with open(os.path.join(diretorio_modelo, 'preprocessor.pkl'), 'rb') as f:\n",
    "        componentes['preprocessor'] = pickle.load(f)\n",
    "    \n",
    "    # Carregando label encoder\n",
    "    with open(os.path.join(diretorio_modelo, 'label_encoder.pkl'), 'rb') as f:\n",
    "        componentes['label_encoder'] = pickle.load(f)\n",
    "    \n",
    "    # Carregando metadados\n",
    "    with open(os.path.join(diretorio_modelo, 'metadata.json'), 'r', encoding='utf-8') as f:\n",
    "        componentes['metadata'] = json.load(f)\n",
    "    \n",
    "    # Carregando bins\n",
    "    with open(os.path.join(diretorio_modelo, 'feature_bins.json'), 'r') as f:\n",
    "        componentes['bins'] = json.load(f)\n",
    "    \n",
    "    return componentes\n",
    "\n",
    "# Fun√ß√£o para fazer predi√ß√µes\n",
    "def predizer_categoria_reclamacao(texto_reclamacao, empresa, estado, valor_envolvido, \n",
    "                                 tempo_resposta_dias, componentes_modelo):\n",
    "    \"\"\"\n",
    "    Faz predi√ß√£o da categoria de uma nova reclama√ß√£o.\n",
    "    \n",
    "    Args:\n",
    "        texto_reclamacao (str): Texto da reclama√ß√£o\n",
    "        empresa (str): Nome da empresa\n",
    "        estado (str): Estado onde ocorreu o problema\n",
    "        valor_envolvido (float): Valor monet√°rio envolvido\n",
    "        tempo_resposta_dias (float): Tempo de resposta em dias\n",
    "        componentes_modelo (dict): Componentes carregados do modelo\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultado da predi√ß√£o com categoria e probabilidades\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Extraindo componentes\n",
    "    modelo = componentes_modelo['modelo']\n",
    "    preprocessor = componentes_modelo['preprocessor']\n",
    "    label_encoder = componentes_modelo['label_encoder']\n",
    "    bins = componentes_modelo['bins']\n",
    "    \n",
    "    # Criando DataFrame com os dados\n",
    "    dados = pd.DataFrame([{\n",
    "        'texto_reclamacao': texto_reclamacao,\n",
    "        'empresa': empresa,\n",
    "        'estado': estado,\n",
    "        'valor_envolvido': valor_envolvido,\n",
    "        'tempo_resposta_dias': tempo_resposta_dias\n",
    "    }])\n",
    "    \n",
    "    # Feature engineering\n",
    "    dados['categoria_valor'] = pd.cut(\n",
    "        dados['valor_envolvido'], \n",
    "        bins=bins['valor_bins'], \n",
    "        labels=['Baixo', 'M√©dio', 'Alto'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    dados['categoria_tempo'] = pd.cut(\n",
    "        dados['tempo_resposta_dias'], \n",
    "        bins=bins['tempo_bins'], \n",
    "        labels=['R√°pido', 'M√©dio', 'Lento'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    dados['tamanho_texto'] = dados['texto_reclamacao'].str.len()\n",
    "    dados['num_palavras'] = dados['texto_reclamacao'].str.split().str.len()\n",
    "    \n",
    "    # Pr√©-processamento\n",
    "    X_processado = preprocessor.transform(dados)\n",
    "    \n",
    "    # Predi√ß√£o\n",
    "    predicao = modelo.predict(X_processado)[0]\n",
    "    probabilidades = modelo.predict_proba(X_processado)[0]\n",
    "    \n",
    "    # Convertendo para label original\n",
    "    categoria_predita = label_encoder.inverse_transform([predicao])[0]\n",
    "    \n",
    "    # Criando dicion√°rio de probabilidades\n",
    "    prob_dict = {}\n",
    "    for i, classe in enumerate(label_encoder.classes_):\n",
    "        prob_dict[classe] = float(probabilidades[i])\n",
    "    \n",
    "    return {\n",
    "        'categoria_predita': categoria_predita,\n",
    "        'confianca': float(probabilidades.max()),\n",
    "        'probabilidades': prob_dict\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes criadas com sucesso!\")\n",
    "print(\"\\nüìã FUN√á√ïES DISPON√çVEIS:\")\n",
    "print(\"1. carregar_modelo_completo() - Carrega todos os componentes\")\n",
    "print(\"2. predizer_categoria_reclamacao() - Faz predi√ß√µes em novos dados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Teste da Fun√ß√£o de Predi√ß√£o\n",
    "\n",
    "Vamos testar se as fun√ß√µes funcionam corretamente carregando o modelo salvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüß™ TESTANDO CARREGAMENTO E USO DO MODELO SALVO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Carregando o modelo salvo\n",
    "print(\"üìÇ Carregando modelo salvo...\")\n",
    "componentes_carregados = carregar_modelo_completo(model_dir)\n",
    "\n",
    "print(\"‚úÖ Modelo carregado com sucesso!\")\n",
    "print(f\"\\nüìä INFORMA√á√ïES DO MODELO CARREGADO:\")\n",
    "metadata = componentes_carregados['metadata']\n",
    "print(f\"   Tipo: {metadata['modelo_tipo']}\")\n",
    "print(f\"   Data de treinamento: {metadata['data_treinamento']}\")\n",
    "print(f\"   Acur√°cia: {metadata['acuracia_teste']:.4f}\")\n",
    "print(f\"   F1-Score: {metadata['f1_score_teste']:.4f}\")\n",
    "print(f\"   Classes: {metadata['classes']}\")\n",
    "\n",
    "# Testando predi√ß√£o com um exemplo\n",
    "print(\"\\nüîÆ TESTANDO PREDI√á√ÉO:\")\n",
    "exemplo_teste = {\n",
    "    'texto_reclamacao': 'Produto chegou quebrado e n√£o funciona direito',\n",
    "    'empresa': 'TechMart',\n",
    "    'estado': 'SP',\n",
    "    'valor_envolvido': 500.0,\n",
    "    'tempo_resposta_dias': 3.0\n",
    "}\n",
    "\n",
    "print(f\"üìã Exemplo de teste:\")\n",
    "for chave, valor in exemplo_teste.items():\n",
    "    print(f\"   {chave}: {valor}\")\n",
    "\n",
    "# Fazendo predi√ß√£o\n",
    "resultado = predizer_categoria_reclamacao(\n",
    "    exemplo_teste['texto_reclamacao'],\n",
    "    exemplo_teste['empresa'],\n",
    "    exemplo_teste['estado'],\n",
    "    exemplo_teste['valor_envolvido'],\n",
    "    exemplo_teste['tempo_resposta_dias'],\n",
    "    componentes_carregados\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ RESULTADO DA PREDI√á√ÉO:\")\n",
    "print(f\"   Categoria: {resultado['categoria_predita']}\")\n",
    "print(f\"   Confian√ßa: {resultado['confianca']:.2%}\")\n",
    "print(f\"   \\nüìä Probabilidades por classe:\")\n",
    "for classe, prob in resultado['probabilidades'].items():\n",
    "    print(f\"      {classe}: {prob:.2%}\")\n",
    "\n",
    "print(f\"\\n‚úÖ TESTE CONCLU√çDO COM SUCESSO!\")\n",
    "print(f\"üéâ O modelo est√° pronto para uso em produ√ß√£o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üìù Resumo e Conclus√µes\n",
    "\n",
    "### üéØ Objetivos Alcan√ßados\n",
    "\n",
    "Este projeto demonstrou um **pipeline completo de Machine Learning** para classifica√ß√£o de reclama√ß√µes de consumidores, abordando todos os aspectos essenciais:\n",
    "\n",
    "### ‚úÖ **Principais Realiza√ß√µes:**\n",
    "\n",
    "1. **üìä An√°lise Explorat√≥ria Completa**\n",
    "   - Dataset balanceado com 4 classes de reclama√ß√µes\n",
    "   - An√°lise detalhada de vari√°veis num√©ricas e categ√≥ricas\n",
    "   - Visualiza√ß√µes interativas para insights\n",
    "\n",
    "2. **üîß Pr√©-processamento Robusto**\n",
    "   - Pipeline automatizado com ColumnTransformer\n",
    "   - Feature Engineering inteligente\n",
    "   - Tratamento adequado de diferentes tipos de dados\n",
    "\n",
    "3. **ü§ñ M√∫ltiplos Modelos Testados**\n",
    "   - 5 algoritmos diferentes comparados\n",
    "   - Valida√ß√£o cruzada estratificada\n",
    "   - Otimiza√ß√£o de hiperpar√¢metros\n",
    "\n",
    "4. **üìà Performance Excelente**\n",
    "   - Modelo final com alta acur√°cia\n",
    "   - M√©tricas balanceadas entre classes\n",
    "   - An√°lise detalhada de erros\n",
    "\n",
    "5. **üîÆ Sistema de Predi√ß√£o Funcional**\n",
    "   - Predi√ß√µes em novos dados\n",
    "   - An√°lise de confian√ßa\n",
    "   - Interpretabilidade dos resultados\n",
    "\n",
    "6. **üíæ Modelo Pronto para Produ√ß√£o**\n",
    "   - Todos os componentes salvos\n",
    "   - Fun√ß√µes para carregamento e uso\n",
    "   - Metadados completos\n",
    "\n",
    "### üìä **Resultados Finais:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä RESUMO FINAL DOS RESULTADOS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Resumo dos melhores resultados\n",
    "print(f\"üèÜ MELHOR MODELO: {model_to_optimize}\")\n",
    "print(f\"\\nüìà PERFORMANCE FINAL:\")\n",
    "print(f\"   Acur√°cia no teste: {test_accuracy_optimized:.4f} ({test_accuracy_optimized*100:.2f}%)\")\n",
    "print(f\"   F1-Score no teste: {test_f1_optimized:.4f}\")\n",
    "print(f\"   CV Score m√©dio: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ MELHORES HIPERPAR√ÇMETROS:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä DISTRIBUI√á√ÉO DE CLASSES NO TESTE:\")\n",
    "for classe in label_encoder.classes_:\n",
    "    count = (y_test == classe).sum()\n",
    "    percentage = count / len(y_test) * 100\n",
    "    print(f\"   {classe}: {count} amostras ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüîç AN√ÅLISE DE PERFORMANCE POR CLASSE:\")\n",
    "for classe in label_encoder.classes_:\n",
    "    precision = classification_rep[classe]['precision']\n",
    "    recall = classification_rep[classe]['recall']\n",
    "    f1 = classification_rep[classe]['f1-score']\n",
    "    print(f\"   {classe}:\")\n",
    "    print(f\"      Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f}\")\n",
    "\n",
    "print(f\"\\nüíæ ARQUIVOS GERADOS:\")\n",
    "arquivos_importantes = [\n",
    "    'reclamacoes_dataset.csv',\n",
    "    'distribuicao_categorias.html',\n",
    "    'analise_variaveis_numericas.html',\n",
    "    'comparacao_modelos.html',\n",
    "    'validacao_cruzada.html',\n",
    "    'matriz_confusao_final.html',\n",
    "    'predicoes_novos_dados.csv',\n",
    "    'modelos_treinados/'\n",
    "]\n",
    "\n",
    "for arquivo in arquivos_importantes:\n",
    "    print(f\"   üìÑ {arquivo}\")\n",
    "\n",
    "print(f\"\\nüéâ PROJETO CONCLU√çDO COM SUCESSO!\")\n",
    "print(f\"\\nüí° PR√ìXIMOS PASSOS SUGERIDOS:\")\n",
    "print(f\"   1. Implementar em API REST para uso em produ√ß√£o\")\n",
    "print(f\"   2. Criar dashboard para monitoramento\")\n",
    "print(f\"   3. Implementar retreinamento autom√°tico\")\n",
    "print(f\"   4. Adicionar mais features de texto (NLP avan√ßado)\")\n",
    "print(f\"   5. Testar com dados reais de reclama√ß√µes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì **Conceitos e Boas Pr√°ticas Demonstradas:**\n",
    "\n",
    "1. **üìä An√°lise Explorat√≥ria de Dados (EDA)**\n",
    "   - Verifica√ß√£o de balanceamento de classes\n",
    "   - An√°lise de correla√ß√µes\n",
    "   - Detec√ß√£o de outliers\n",
    "   - Visualiza√ß√µes informativas\n",
    "\n",
    "2. **üîß Pr√©-processamento Profissional**\n",
    "   - Divis√£o estratificada dos dados\n",
    "   - Pipeline automatizado\n",
    "   - Feature Engineering\n",
    "   - Tratamento de diferentes tipos de dados\n",
    "\n",
    "3. **ü§ñ Modelagem Robusta**\n",
    "   - Compara√ß√£o de m√∫ltiplos algoritmos\n",
    "   - Valida√ß√£o cruzada estratificada\n",
    "   - Otimiza√ß√£o de hiperpar√¢metros\n",
    "   - An√°lise de overfitting\n",
    "\n",
    "4. **üìà Avalia√ß√£o Completa**\n",
    "   - M√∫ltiplas m√©tricas de avalia√ß√£o\n",
    "   - Matriz de confus√£o detalhada\n",
    "   - An√°lise por classe\n",
    "   - Interpreta√ß√£o dos resultados\n",
    "\n",
    "5. **üîÆ Predi√ß√£o e Interpretabilidade**\n",
    "   - Predi√ß√µes com an√°lise de confian√ßa\n",
    "   - Import√¢ncia das features\n",
    "   - Interpreta√ß√£o dos resultados\n",
    "   - Casos de uso pr√°ticos\n",
    "\n",
    "6. **üíæ Produ√ß√£o e Deployment**\n",
    "   - Salvamento de modelos e transformadores\n",
    "   - Fun√ß√µes para carregamento\n",
    "   - Metadados completos\n",
    "   - Testes de funcionamento\n",
    "\n",
    "### üåü **Valor do Projeto:**\n",
    "\n",
    "Este notebook serve como um **template completo** para projetos de classifica√ß√£o em Machine Learning, demonstrando:\n",
    "\n",
    "- ‚úÖ **Metodologia cient√≠fica** rigorosa\n",
    "- ‚úÖ **Boas pr√°ticas** da ind√∫stria\n",
    "- ‚úÖ **C√≥digo reproduz√≠vel** e documentado\n",
    "- ‚úÖ **Visualiza√ß√µes profissionais**\n",
    "- ‚úÖ **Sistema pronto para produ√ß√£o**\n",
    "\n",
    "O projeto pode ser facilmente adaptado para outros contextos de classifica√ß√£o, mantendo a mesma estrutura e metodologia robusta.\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Desenvolvido como material did√°tico para demonstrar um pipeline completo de Machine Learning para classifica√ß√£o multiclasse em contexto administrativo/servi√ßo p√∫blico.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
